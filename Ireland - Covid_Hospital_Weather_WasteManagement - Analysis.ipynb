{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATABASE & ANALYTICS PROGRAMMING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scraping and storing the file as CSV\n",
    "def fetchRequiredCSV():\n",
    "    \"\"\" \n",
    "    This function will scrape required content from web.\n",
    "    Input: Input is static as we are scrapping only 1 fixed link to identify our required file.\n",
    "    Output: Output will be csv file containing data of year 2020 stored locally.\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    # Below brings all the content of given web page.\n",
    "    response = requests.get('https://data.ehealthireland.ie/dataset/general-referrals-by-hospital-department-and-year')\n",
    "    # html_soup will parse content of webpage in html format.\n",
    "    html_soup = BeautifulSoup(response.content, features=\"html.parser\")\n",
    "    # Below allAnchorTags will fetch all anchor tags data in the form of list.\n",
    "    allAnchorTags = html_soup.find_all('a')\n",
    "    # I am now going to loop on each element of anchor tags list.\n",
    "    for aTag in allAnchorTags:\n",
    "        # Below will fetch content of href from each anchor tag.\n",
    "        link = aTag.attrs['href']\n",
    "        # Now, I am checking if my found link has .csv file or link to other page.\n",
    "        # In case it is link to other page, I will simply ignore it.\n",
    "        if link.find(\".csv\") != -1:\n",
    "            # In case link is .csv file name, read csv file.\n",
    "            file_stream = requests.get(link,stream=True)\n",
    "            # Now, I will loop through all contents of csv file.\n",
    "            for record in file_stream:\n",
    "                # I will now decode csv data as it will be in byte stream encoded by utf8 format.\n",
    "                data = record.decode('utf8', errors='ignore')\n",
    "                # In case my data contains date with year 2020, it is required file.\n",
    "                if data.find(\"-2020\") != -1:\n",
    "                    # I am storing file name to be used later on.\n",
    "                    requiredFileName = link\n",
    "                    break\n",
    "    # I am opening a new file locally to write content of above identified file.\n",
    "    with open('Hospital_data.csv', 'wb') as fileWritter:\n",
    "        # Fetching all data using file stream.\n",
    "        file_stream = requests.get(requiredFileName,stream=True)\n",
    "        # Looping on each line of file.\n",
    "        for record in file_stream.iter_content(chunk_size=1024 * 1024):\n",
    "            # Writing content of file locally.\n",
    "            fileWritter.write(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting CSV to JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csvToJson():\n",
    "    \"\"\" \n",
    "    This function will convert CSV data into JSON data.\n",
    "    Input: Input is static CSV file which is web scrapped.\n",
    "    Output: Output will be JSON file containing JSON data.\n",
    "    \"\"\"\n",
    "    # Importing necessary libraries\n",
    "    import csv\n",
    "    import json\n",
    "    # Opening CSV file which needs to be converted in read only mode.\n",
    "    f = open('Hospital_data.csv', 'r')\n",
    "    # Creating a CSV reader\n",
    "    reader = csv.DictReader(f) \n",
    "    # Opening JSON file in write mode to write the data in it.\n",
    "    with open('Hospital_data.json', 'w') as f:\n",
    "        # Looping for all the record in CSV file\n",
    "        for i in reader: \n",
    "            # Creating dump in json format\n",
    "            json.dump(i,f)\n",
    "            # Writing in JSON file\n",
    "            f.write('\\n')\n",
    "    # Opening JSON file in read mode, reading line by line and storing JSON records in variable data\n",
    "    json_data = [json.loads(line) for line in open('Hospital_data.json', 'r')]\n",
    "    # Returning json data to store in MongoDB\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing JSON data in MongoDB and Retrieving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Storing raw JSON data into MongoDB\n",
    "def opnMongoDB(json_data):\n",
    "    \"\"\" \n",
    "    This function will store JSON data into MongoDB and will fetch data from MongoDB into a dataframe.\n",
    "    Input: Input is Json data which needs to be stored in database.\n",
    "    Output: Output is dataframe having data retrieved from MongoDB.\n",
    "    \"\"\"\n",
    "    # Importing necessary libraries\n",
    "    from pymongo import MongoClient\n",
    "    import pandas as pd\n",
    "    # Making a Connection with MongoClient\n",
    "    client = MongoClient('192.168.56.30', 27017)\n",
    "    # Creating a client for MongoDB\n",
    "    db = client['Client_DAPProj']\n",
    "    # Creating a database in MongoDB\n",
    "    hos_collection = db['Hospital_DB']\n",
    "    # Deleting all old records from database if any exists before inserting new records.\n",
    "    db.Hospital_DB.delete_many({ });\n",
    "    # Inserting records into MongoDB\n",
    "    result = hos_collection.insert_many(json_data)\n",
    "    # Creating a cursor\n",
    "    hospital_cursor = hos_collection.find()\n",
    "    # Creating a datalist\n",
    "    hospital_datalist = list(hospital_cursor)\n",
    "    # Storing datalist as dataframe\n",
    "    hospital_df = pd.DataFrame(hospital_datalist)\n",
    "    # Returning data in dataframe\n",
    "    return hospital_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions required in Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if any special character is present and excluding it.\n",
    "def CheckString(GivenString):\n",
    "    \"\"\" \n",
    "    This function will check if any special character is present and will exclude it if any.\n",
    "    Input: Input is String or Pandas Dataframe columns which can be given as Pandas Series.\n",
    "    Output: Output will be cleaned String with no special characters. \n",
    "    \"\"\"\n",
    "    # Initialising temp string.\n",
    "    TempString = \"\"\n",
    "    # Looping for length of given string.\n",
    "    for i in GivenString:\n",
    "        # assigning integer value of current character to variable.\n",
    "        value = ord(i)\n",
    "        # In case assigned character is a-z or A-Z or space, do as below.\n",
    "        if(value == 32 or (value>64 and value<91) or (value>96 and value<123)):\n",
    "            # Add current character to temperory memory. This will be finally returned once all characters of string are checked.\n",
    "            TempString = TempString + i\n",
    "    # Returning the cleaned string.\n",
    "    return TempString "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert Month column which has names of the month to numeric.\n",
    "def month_to_num(month):\n",
    "    \"\"\" \n",
    "    This function will convert string Month to numeric.\n",
    "    Input: Input is string Month or Pandas Dataframe columns which can be given as Pandas Series.\n",
    "    Output: Output will be numeric Month\n",
    "    \"\"\"\n",
    "    # Creating a List of all months\n",
    "    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    # Returning Index of month + 1 which gives the exact numeric value for that month\n",
    "    return months.index(month) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataCleaning(hospital_df):\n",
    "    \"\"\" \n",
    "    This function will clean the input dataframe.\n",
    "    Input: Input is a dataframe which needs to be cleaned.\n",
    "    Output: Output will be cleaned dataframe.\n",
    "    \"\"\"\n",
    "    # Separating Month_Year column\n",
    "    hospital_df[['Month','Year']] = hospital_df.Month_Year.str.split(\"-\", expand=True)\n",
    "    # Separating Month_Year column\n",
    "    hospital_df[['Month','Year']] = hospital_df.Month_Year.str.split(\"-\", expand=True)\n",
    "    # Dropping redundant columns\n",
    "    hospital_df = hospital_df.drop(['_id','Month_Year','Hospital_ID','Year'], axis=1)\n",
    "    # Replacing '\\' char with blank space in Hospital_Department and Hospital Name columns\n",
    "    hospital_df['Hospital_Department'] = hospital_df['Hospital_Department'].replace({'\\'': ''}, regex=True)\n",
    "    hospital_df['Hospital_Name'] = hospital_df['Hospital_Name'].replace({'\\'': ''}, regex=True)\n",
    "    # Applying the Check String function on all the columns of the dataframe having string values .\n",
    "    hospital_df[\"Hospital_Department\"] = hospital_df[\"Hospital_Department\"].apply(CheckString) \n",
    "    hospital_df[\"Hospital_Name\"] = hospital_df[\"Hospital_Name\"].apply(CheckString) \n",
    "    hospital_df[\"ReferralType\"] = hospital_df[\"ReferralType\"].apply(CheckString)\n",
    "    # Applying Month to number conversion function on Month column\n",
    "    hospital_df[\"Month\"] = hospital_df[\"Month\"].apply(month_to_num) \n",
    "    # Resetting index in order to add Unique column in the dataframe\n",
    "    hospital_df = hospital_df.reset_index()\n",
    "    # Dropping extra index column\n",
    "    hospital_df = hospital_df.drop(['index'], axis = 1)\n",
    "    # Renaming index column as Unique Column\n",
    "    hospital_df.index.rename('Unique_col', inplace=True)\n",
    "    # Again resetting index to bring it in proper format\n",
    "    hospital_df.reset_index(inplace=True)\n",
    "    # Renaming Columns of the Hospital dataframe\n",
    "    hospital_df.rename(columns = {'Hospital_Department': 'Hosp_Dept','ReferralType':'RefType','TotalReferrals':'TotalRef'}, \n",
    "                       inplace=True)\n",
    "    # Converting the data type of Total Referral column to integer\n",
    "    hospital_df['TotalRef'] = hospital_df['TotalRef'].astype('int64')\n",
    "    # Storing Cleaned CSV file locally to store in POSTGRESQL database\n",
    "    hospital_df.to_csv(\"HospitalRefData.csv\", index=None)\n",
    "    # Returning cleaned dataframe\n",
    "    return hospital_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Cleaned Data into PostgreSQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Connecting to Postgre database and Creating new database.\n",
    "def createDB():\n",
    "    \"\"\" \n",
    "    This function will connect to PostgreSQL database and will create new database.\n",
    "    Input: Input is the SQL query to create database.\n",
    "    Output: The database will be created in PostgreSQL\n",
    "    \"\"\"\n",
    "    # Importing necessary libraries\n",
    "    import psycopg2\n",
    "    try:\n",
    "        # Making a connection with PostgreSQL database\n",
    "        dbConnection = psycopg2.connect(\n",
    "            user = \"dap\",\n",
    "            password = \"dap\",\n",
    "            host = \"192.168.56.30\",\n",
    "            port = \"5432\",\n",
    "            database = \"postgres\")\n",
    "        # Below statement is for autocommit\n",
    "        dbConnection.set_isolation_level(0) \n",
    "        # Creating a DB cursor\n",
    "        dbCursor = dbConnection.cursor()\n",
    "        # Dropping database if already present\n",
    "        dbCursor.execute(\"DROP DATABASE IF EXISTS DAPProject;\")\n",
    "        # Executing SQL statement using DB cursor\n",
    "        dbCursor.execute('CREATE DATABASE DAPProject;')\n",
    "        # Closing the DB cursor\n",
    "        dbCursor.close()\n",
    "    except (Exception , psycopg2.Error) as dbError :\n",
    "        # Printing error if occurred while connecting to PostgreSQL database\n",
    "        print (\"Error while connecting to PostgreSQL\", dbError)\n",
    "    finally:\n",
    "        # Finally close the DB connection\n",
    "        if(dbConnection): dbConnection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting and inserting records in Table created from Cleaned CSV file.\n",
    "def insertData():\n",
    "    \"\"\" \n",
    "    This function will connect to PostgreSQL database, create new table and insert values.\n",
    "    Input: Input is the SQL query to create table and insert values.\n",
    "    Output: The table will be created in PostgreSQL and values will be inserted.\n",
    "    \"\"\"\n",
    "    # Importing necessary libraries\n",
    "    import psycopg2\n",
    "    import csv\n",
    "    import os\n",
    "    # Setting path of current working directory\n",
    "    path = os.getcwd()\n",
    "    # SQL statement to create table in database.\n",
    "    createtblString = \"\"\"\n",
    "    CREATE TABLE Hospital_Table(\n",
    "    Unique_col int PRIMARY KEY, \n",
    "    hospital_name varchar(255),\n",
    "    hospital_dept varchar(255),\n",
    "    reftype varchar(255),\n",
    "    totalref int,\n",
    "    month int\n",
    "    );\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Making a connection with PostgreSQL database\n",
    "        dbConnection = psycopg2.connect(\n",
    "            user = \"dap\",\n",
    "            password = \"dap\",\n",
    "            host = \"192.168.56.30\",\n",
    "            port = \"5432\",\n",
    "            database = \"dapproject\")\n",
    "        # Below statement is for autocommit\n",
    "        dbConnection.set_isolation_level(0) \n",
    "        # Creating a DB cursor\n",
    "        dbCursor = dbConnection.cursor()\n",
    "        # Executing SQL statement using DB cursor, first dropping the table if already exists \n",
    "        dbCursor.execute(\"DROP TABLE IF EXISTS Hospital_Table;\")\n",
    "        # Executing SQL statement using DB cursor, creating table in database \n",
    "        dbCursor.execute(createtblString)\n",
    "        # Inserting records in table\n",
    "        insertStr = \"INSERT INTO Hospital_Table VALUES ({}\"+\",'{}',\"+\"'{}',\"+\"'{}',\"+\"{},\"+\"{})\"\n",
    "        # Opening Cleaned CSV file in read mode\n",
    "        with open(path + '\\HospitalRefData.csv','r') as f:\n",
    "            # Reading each record from CSV file using CSV reader\n",
    "            reader = csv.reader(f)\n",
    "            # skipping the header\n",
    "            next(reader) \n",
    "            # For each record in CSV file \n",
    "            for row in reader:\n",
    "                # Inserting record in table using DB cursor\n",
    "                dbCursor.execute(insertStr.format(*row))\n",
    "        # Commiting the transaction using DB connection        \n",
    "        dbConnection.commit()\n",
    "        # Closing the DB cursor\n",
    "        dbCursor.close()\n",
    "    except (Exception , psycopg2.Error) as dbError :\n",
    "        # Printing error if occurred while connecting to PostgreSQL database\n",
    "        print (\"Error:\", dbError)\n",
    "    finally:\n",
    "        # Finally close the DB connection\n",
    "        if(dbConnection): dbConnection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opnPostgre():\n",
    "    \"\"\" \n",
    "    This function will connect to PostgreSQL database, create new database, new table and insert values.\n",
    "    Input: Input is the SQL query to create database, table and insert values.\n",
    "    Output: The database and table will be created in PostgreSQL and values will be inserted.\n",
    "    \"\"\"\n",
    "    # To create new database in postgreSQL.\n",
    "    createDB()\n",
    "    # To create new table and insert values in database.\n",
    "    insertData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fetching data from postgresql database and storing in dataframe for visualization\n",
    "def dv12():\n",
    "    \"\"\" \n",
    "    This function will connect to PostgreSQL database, perform sql operations and visualize data.\n",
    "    Input: Input is the SQL query to get data in dataframe\n",
    "    Output: Visualize data from dataframe\n",
    "    \"\"\"\n",
    "    # Importing necessary libraries\n",
    "    import matplotlib.pyplot as plt\n",
    "    from plotly.offline import iplot\n",
    "    import pandas.io.sql as sqlio\n",
    "    import seaborn as sns\n",
    "    import psycopg2\n",
    "    # Creating SQL statement \n",
    "    sql1 = \"\"\" SELECT *  FROM Hospital_Table where reftype = 'Covid Testing Referral';\"\"\"\n",
    "    try:\n",
    "        # Making a connection with PostgreSQL database\n",
    "        dbConnection = psycopg2.connect(\n",
    "            user = \"dap\",\n",
    "            password = \"dap\",\n",
    "            host = \"192.168.56.30\",\n",
    "            port = \"5432\",\n",
    "            database = \"dapproject\")\n",
    "        # Executing the created SQL statement using DB connection\n",
    "        df1 = sqlio.read_sql_query(sql1, dbConnection)\n",
    "    except (Exception , psycopg2.Error) as dbError :\n",
    "        # Printing error if occurred while connecting to PostgreSQL database\n",
    "        print (\"Error:\", dbError)\n",
    "    finally:\n",
    "        # Finally close the DB connection\n",
    "        if(dbConnection): dbConnection.close()\n",
    "    \n",
    "    # Creating SQL statement 2\n",
    "    sql2 = \"\"\" SELECT totalref, reftype FROM Hospital_Table;\"\"\"\n",
    "    try:\n",
    "        # Making a connection with PostgreSQL database\n",
    "        dbConnection = psycopg2.connect(\n",
    "            user = \"dap\",\n",
    "            password = \"dap\",\n",
    "            host = \"192.168.56.30\",\n",
    "            port = \"5432\",\n",
    "            database = \"dapproject\")\n",
    "        # Executing the created SQL statement using DB connection\n",
    "        df2 = sqlio.read_sql_query(sql2, dbConnection)\n",
    "    except (Exception , psycopg2.Error) as dbError :\n",
    "        # Printing error if occurred while connecting to PostgreSQL database\n",
    "        print (\"Error:\", dbError)\n",
    "    finally:\n",
    "        # Finally close the DB connection\n",
    "        if(dbConnection): dbConnection.close()      \n",
    "\n",
    "    ## Data Visualization 1\n",
    "    # Creating a barplot of Month and Total Referrals.\n",
    "    sns.barplot(x=\"month\", y=\"totalref\", data=df1)\n",
    "    # Labelling X-axis\n",
    "    plt.xlabel(\"Month\")\n",
    "    # Labelling X-axis\n",
    "    plt.ylabel(\"Total referrals in month\")\n",
    "    # Giving a title to the figure\n",
    "    plt.title(\"Plot 1: Total Covid-19 referrals in a month\")\n",
    "    # To Display the figure\n",
    "    plt.show()\n",
    "    \n",
    "    ## Data Visualization 2\n",
    "    # Creating a pie chart for Referral Type\n",
    "    fig1 = {\n",
    "      \"data\": [{\"values\": list(df2.totalref), \"labels\": df2.reftype, \"domain\": {\"x\": [0, .5]}, \"name\": \"TotalReferrals\",\n",
    "                \"hoverinfo\":\"label+percent\", \"hole\": .3, \"type\": \"pie\"}], \n",
    "                \"layout\": {\"title\":\"Plot 2: Percentage of each Referral Type\",\n",
    "                \"annotations\": [{\"font\": {\"size\": 20}, \"showarrow\": False, \"text\": \"Total Referrals\", \"x\": 0.20, \"y\": 1}]}\n",
    "    }\n",
    "    # To display the figure\n",
    "    iplot(fig1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fetching data from postgresql database and storing in dataframe for visualization\n",
    "def dv34(hospital_df):\n",
    "    \"\"\" \n",
    "    This function will connect to PostgreSQL database, perform sql operations and visualize data.\n",
    "    Input: Input is the SQL query to get data in dataframe\n",
    "    Output: Visualize data from dataframe\n",
    "    \"\"\"\n",
    "    # Importing necessary libraries\n",
    "    from skimage import io\n",
    "    from wordcloud import WordCloud\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas.io.sql as sqlio\n",
    "    import plotly.express as px\n",
    "    import psycopg2\n",
    "    import os\n",
    "    # Setting path of current working directory\n",
    "    path = os.getcwd()\n",
    "    # Creating SQL statement \n",
    "    sql = \"\"\" SELECT * FROM Hospital_Table where hospital_name='A Covid  Testing Service';\"\"\"\n",
    "    try:\n",
    "        # Making a connection with PostgreSQL database\n",
    "        dbConnection = psycopg2.connect(\n",
    "            user = \"dap\",\n",
    "            password = \"dap\",\n",
    "            host = \"192.168.56.30\",\n",
    "            port = \"5432\",\n",
    "            database = \"dapproject\")\n",
    "        # Executing the created SQL statement using DB connection\n",
    "        df3 = sqlio.read_sql_query(sql, dbConnection)\n",
    "    except (Exception , psycopg2.Error) as dbError :\n",
    "        # Printing error if occurred while connecting to PostgreSQL database\n",
    "        print (\"Error:\", dbError)\n",
    "    finally:\n",
    "        # Finally close the DB connection\n",
    "        if(dbConnection): dbConnection.close()\n",
    "            #     url = \"C:/Users/Prasad/Desktop/DAP/Project/corona.png\"\n",
    "    ## Data Visualization 3\n",
    "    # Storing the path for corona mask image\n",
    "    url = path + '\\corona.png'\n",
    "    # Reading the URL path and storing mask in mask variable\n",
    "    mask = io.imread(url)\n",
    "    # Generating wordcloud object\n",
    "    wc_all_words=WordCloud(background_color=\"white\", width=400, height=384, colormap='Dark2',\n",
    "                           random_state=21, mask=mask,contour_width=1,max_font_size=200).generate(\" \".join(df3.hospital_dept))\n",
    "    # Plotting wordcloud\n",
    "    plt.figure(figsize=(10,10)) # Specifying height and width for figure\n",
    "    plt.imshow(wc_all_words, interpolation='bilinear') # To show the figure with the wordcloud object created \n",
    "    plt.title(\"Plot 3: Word cloud for Hospital Departments\")\n",
    "    plt.axis('off') # To turn off axes in plot\n",
    "    plt.show() # To display the figure\n",
    "    \n",
    "    ## Data Visualization 4\n",
    "    # Creating a scatter plot between Total Referrals and Hospital Department with Referral Type\n",
    "    fig4 = px.scatter(hospital_df, x = 'TotalRef', y = 'Hosp_Dept', color = 'RefType', log_x=True, range_x=[10,4000], \n",
    "                     hover_name='Hospital_Name', animation_frame='Month', animation_group='Hosp_Dept')\n",
    "    # Adding title to the figure and labelling x and y axis\n",
    "    fig4.update_layout(title = 'Plot 4: Scatterplot of TotalReferrals and Hospital Department',\n",
    "                      xaxis_title = 'Total Referrals', yaxis_title = 'Hospital Department')\n",
    "    # To display the figure\n",
    "    fig4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataVisualization(hospital_df):\n",
    "    \"\"\" \n",
    "    This function will connect to PostgreSQL database, perform sql operations and visualize data.\n",
    "    Input: Input is the SQL query to get data in dataframe and cleaned dataframe\n",
    "    Output: Visualize data from dataframe\n",
    "    \"\"\"\n",
    "    # Perform Data visualization 1 and 2\n",
    "    dv12()\n",
    "    # Perform Data visualization 3 and 4\n",
    "    dv34(hospital_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Details of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statisticalDetails(hospital_df):\n",
    "    \"\"\" \n",
    "    This function is to check the statistical details like mean, quartile, etc.\n",
    "    Input: Input is final cleaned dataframe.\n",
    "    Output: Output will be statistical measures and Pearson's Correlation Coefficient between all numerical features.\n",
    "    \"\"\"\n",
    "    # Dropping Unique values column as it is not required now\n",
    "    hospital_df = hospital_df.drop(['Unique_col'], axis = 1)\n",
    "    # Checking some statistical details such as mean, percentile, etc.\n",
    "    print(\"Statistical details of Hospital dataset\")\n",
    "    print(\"------------------------------------------\")\n",
    "    print(hospital_df.describe())\n",
    "    #Checking Pearson's correlation between Month and TotalReferrals.\n",
    "    print(\"\\n Pearson's Correlation Coefficient for all numerical features present in Hospital dataset\")\n",
    "    print(\"-----------------------------------------------------------------------------------------\")\n",
    "    print(hospital_df.corr()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataPrep(hospital_df):\n",
    "    \"\"\" \n",
    "    This function is to prepare Hospital dataset as per the requirement for joining.\n",
    "    Input: Input is final cleaned dataframe. \n",
    "    Output: Output will be dataframe ready to be joined.\n",
    "    \"\"\"\n",
    "    # Creating a copy cleaned hospital dataframe\n",
    "    hosp_df = hospital_df.copy()\n",
    "    # Dropping redundant column\n",
    "    hosp_df = hosp_df.drop(['Unique_col'], axis = 1)\n",
    "    # Filtering Hospital name with COVID testing service as we are interested in these instances only.\n",
    "    hosp_df = hosp_df.loc[hosp_df['Hospital_Name'] == 'A Covid  Testing Service']\n",
    "    # Dropping Hospital Name column as it is not required now\n",
    "    hosp_df = hosp_df.drop(['Hospital_Name'], axis=1)\n",
    "    # Merging multiple hospital departments into few departments\n",
    "    hosp_df['Hosp_Dept'] = hosp_df['Hosp_Dept'].replace(['j Healthcare Worker Unable To Travel',\n",
    "                                               'k Close Contact of Cfirmed Case Unable To Travel',\n",
    "                                               'l At Risk Group Unable To Travel',\n",
    "                                               'm Hhold Contact At Risk Group Unable To Travel',\n",
    "                                               'p Prison Staff Unable To Travel',\n",
    "                                               'q Pregnant Woman Unable To Travel', 'Unfit to Travel',\n",
    "                                               'oHhold contact of Hcare Worker Unable to Travel',\n",
    "                                               'n General Covid  Test Unable to Travel',\n",
    "                                               'f Unable to Attend Clinic'],'Unable to travel')\n",
    "    hosp_df['Hosp_Dept'] = hosp_df['Hosp_Dept'].replace(['Essential Worker','a Healthcare Worker','h Prison Staff',\n",
    "                                              'g Household contact of Healthcare Worker'],'Essential Individuals')\n",
    "    hosp_df['Hosp_Dept'] = hosp_df['Hosp_Dept'].replace(['General Covid  Testing','Covid Testing Service',\n",
    "                                  'e General Covid  Test'],'General Covid Test')\n",
    "    hosp_df['Hosp_Dept'] = hosp_df['Hosp_Dept'].replace(['i Pregnant Woman','Residential Setting','Prison Inmate'],'Others')\n",
    "    hosp_df['Hosp_Dept'] = hosp_df['Hosp_Dept'].replace(['c At Risk Group','f Household Contact of At Risk Group',\n",
    "                                               'b Close Contact of Confirmed Case'],'Risk Group Individuals')\n",
    "    # Grouping by Hospital Department, Referral Type, Month, taking sum of total referrals and sorting values by Month\n",
    "    hosp_df = hosp_df.groupby(['Hosp_Dept','RefType','Month']).agg({'TotalRef': 'sum'}).sort_values(by=['Month']).reset_index()\n",
    "    # Saving final cleaned dataframe in CSV file for merging.\n",
    "    hosp_df.to_csv('CleanedHospitalData.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function of the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainExecute():\n",
    "    \"\"\" \n",
    "    This is the main function from where the execution of program starts.\n",
    "    Input: Input is CSV file which is web scraped.\n",
    "    Output: Output will be analysis and visulizations performed on cleaned dataframe.\n",
    "    \"\"\"\n",
    "    fetchRequiredCSV()\n",
    "    jdata = csvToJson()\n",
    "    hos_df = opnMongoDB(jdata)\n",
    "    clean_df = dataCleaning(hos_df)\n",
    "    opnPostgre()\n",
    "    dataVisualization(clean_df)\n",
    "    statisticalDetails(clean_df)\n",
    "    dataPrep(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the main function. This is point from where the execution of this program starts.\n",
    "mainExecute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ Insights: \n",
    "- Plot 1: We can see that in the month of October there were maximum Covid Testing Referrals done followed by September and November. Also we can notice that this type of referral was started from June 2020 only before that there were no referrals done of this type.\n",
    "- Plot 2: We can infer that 53.5% of the total referrals were made under General Referral Category followed by 42.1% of Covid Testing Referrals. The Breast Clinic Referral was third with 2.21% of referrals which is also very low as compared to the first two.\n",
    "- Plot 3: We can see the words like Risk Group, Travel, Healthcare Worker, Covid Testing, Close Contact and so on. These words are related to COVID-19. These were the main departments under 'A Covid Testing Service' hospital.\n",
    "- Plot 4: We can infer that maximum patients come under General Referrals category followed by Covid Testing Referrals irrespective of hospital departments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting file from webpage by Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author - Prasad Rudrappa Shivu (Student Id: 19213077)\n",
    "# COVID-19 Dataset\n",
    "#Webscraping the data file\n",
    "\n",
    "#Import urllib library to webscrape\n",
    "from urllib.request import urlretrieve as retrieve\n",
    "\n",
    "#Assign URL of the online data to an object\n",
    "url='https://opendata-geohive.hub.arcgis.com/datasets/d9be85b30d7748b5b7c09450b8aede63_0.csv?outSR=%7B%22latestWkid%22%3A3857%2C%22wkid%22%3A102100%7D'\n",
    "\n",
    "#Retrieve the file from the URL\n",
    "retrieve(url,'Covid19CountyStatisticsHPSCIreland.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert CSV to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the csv file to json\n",
    "\n",
    "#Import the libraries required - json and csv\n",
    "import csv \n",
    "import json \n",
    "\n",
    "#function to convert csv to json\n",
    "def csv_to_json(csvFilePath, jsonFilePath):\n",
    "    jsonArray = []\n",
    "\n",
    "#read csv file\n",
    "    with open(csvFilePath, encoding='utf-8') as csvf:\n",
    "\n",
    "#load csv file data using csv library's dictionary reader\n",
    "        csvReader = csv.DictReader(csvf)\n",
    "\n",
    "#convert each csv row into python dict\n",
    "        for row in csvReader:\n",
    "\n",
    "#add this python dict to json array\n",
    "            jsonArray.append(row)\n",
    "    \n",
    "#convert python jsonArray to JSON String and write to file\n",
    "    with open(jsonFilePath, 'w', encoding='utf-8') as jsonf: \n",
    "        jsonString = json.dumps(jsonArray, indent=4)\n",
    "        jsonf.write(jsonString)\n",
    "          \n",
    "#provide filenames to the parameters\n",
    "csvFilePath = r'Covid19CountyStatisticsHPSCIreland.csv'\n",
    "jsonFilePath = r'Covid19CountyStatisticsHPSCIreland.json'            \n",
    "\n",
    "#call the function\n",
    "csv_to_json(csvFilePath, jsonFilePath)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load JSON file to an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#Assign the json file to an object\n",
    "jobj1 = open(r\"Covid19CountyStatisticsHPSCIreland.json\")\n",
    "\n",
    "#Load the json data\n",
    "jdata1 = json.load(jobj1)\n",
    "\n",
    "#display the file contents\n",
    "jdata1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing pymongo\n",
    "from pymongo import MongoClient\n",
    "import pymongo as pym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to mongodb\n",
    "client = MongoClient('192.168.56.30', 27017)\n",
    "\n",
    "#creating client for mongodb\n",
    "db = client['DAPPro']\n",
    "\n",
    "#creating db in mongodb\n",
    "cov_collection = db['CovidDB']\n",
    "\n",
    "db.CovidDB.delete_many({ });"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inserting records into DB\n",
    "result = cov_collection.insert_many(jdata1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data into Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas library\n",
    "import pandas as pd\n",
    "\n",
    "#find records in db\n",
    "cov_cursor = cov_collection.find()\n",
    "\n",
    "#creating list of records using cursor\n",
    "cov_datalist = list(cov_cursor)\n",
    "\n",
    "#store list in dataframe\n",
    "cov_df = pd.DataFrame(cov_datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the first five rows of the dataframe\n",
    "cov_df.head()\n",
    "#print the shape of the dataframe\n",
    "cov_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning - replace null with 0, remove unrequired colums and insert new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the unrequired columns\n",
    "cov_df = cov_df.drop(['_id','IGEasting','ORIGID','IGNorthing','UGI','ConfirmedCovidDeaths','ConfirmedCovidRecovered','Shape__Area','Shape__Length'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill null values with NaN\n",
    "\n",
    "#import numpy library\n",
    "import numpy as np\n",
    "\n",
    "#replace empty spaces with NaN\n",
    "cov_df=cov_df.replace(\"\",np.NaN)\n",
    "\n",
    "# here we are searching for white spaces and replacing it with NaN by the help of numpy\n",
    "cov_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill NaN places with 0\n",
    "cov_df=cov_df.fillna(0)\n",
    "#display first five rows of the dataframe\n",
    "cov_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new 'date' column by separating date from timestamp as time values are 0 throughout\n",
    "cov_df['Date'] = cov_df['TimeStamp'].str[:10]\n",
    "#extract month from the date column\n",
    "cov_df['Month'] = pd.DatetimeIndex(cov_df['Date']).month\n",
    "#drop the column timestamp\n",
    "del cov_df['TimeStamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display first five rows of the dataframe\n",
    "cov_df.head()\n",
    "cov_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to postgreSQL, and Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data in cov_df dataframe to new CSV file\n",
    "cov_df.to_csv('CleanedCovidData.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to PostgreSQL and create empty database, coviddata\n",
    "\n",
    "#import library\n",
    "import psycopg2\n",
    "\n",
    "try:\n",
    "    #establish connecting by giving user, password, host IP, and DB name\n",
    "    dbConnection = psycopg2.connect(\n",
    "        user = \"dap\",\n",
    "        password = \"dap\",\n",
    "        host = \"192.168.56.30\",\n",
    "        port = \"5432\",\n",
    "        database = \"postgres\")\n",
    "    \n",
    "    #set isolation level to 0 / autocommit\n",
    "    dbConnection.set_isolation_level(0)\n",
    "    \n",
    "    #establish cursor connection\n",
    "    dbCursor = dbConnection.cursor()\n",
    "    dbCursor.execute(\"DROP DATABASE IF EXISTS covidData1;\")\n",
    "    #create datbase\n",
    "    dbCursor.execute(\"CREATE DATABASE covidData1;\")\n",
    "    \n",
    "    #close the cursor\n",
    "    dbCursor.close()\n",
    "\n",
    "#throw except if connection is not established\n",
    "except (Exception , psycopg2.Error) as dbError :\n",
    "    print (\"Error while connecting to PostgreSQL\", dbError)\n",
    "\n",
    "#close the connection\n",
    "finally:\n",
    "    if(dbConnection): dbConnection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create table in database\n",
    "createString = \"\"\"\n",
    "CREATE TABLE covidtablenew(\n",
    "OBJECTID integer,\n",
    "CountyName varchar(255),\n",
    "PopulationCensus16 numeric(15,1),\n",
    "Lat float,\n",
    "Long float,\n",
    "ConfirmedCovidCases numeric(15,1),\n",
    "PopulationProportionCovidCases float,\n",
    "Date varchar(50),\n",
    "Month integer\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    #establish connecting by giving user, password, host IP, and DB name\n",
    "    dbConnection = psycopg2.connect(\n",
    "        user = \"dap\",\n",
    "        password = \"dap\",\n",
    "        host = \"192.168.56.30\",\n",
    "        port = \"5432\",\n",
    "        database = \"coviddata1\")\n",
    "    \n",
    "    #set isolation level to 0 / autocommit\n",
    "    dbConnection.set_isolation_level(0)\n",
    "    \n",
    "    #establish cursor connection\n",
    "    dbCursor = dbConnection.cursor()\n",
    "    \n",
    "    #drop the table if preexists\n",
    "    dbCursor.execute(\"DROP TABLE IF EXISTS covidtablenew\")\n",
    "    \n",
    "    #execute the create table query\n",
    "    dbCursor.execute(createString)\n",
    "    \n",
    "    #close the cursor\n",
    "    dbCursor.close()\n",
    "    \n",
    "#throw except if connection is not established\n",
    "except (Exception , psycopg2.Error) as dbError :\n",
    "    print (\"Error while connecting to PostgreSQL\", dbError)\n",
    "\n",
    "#close the connection    \n",
    "finally:\n",
    "    if(dbConnection): dbConnection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #establish connecting by giving user, password, host IP, and DB name\n",
    "    dbConnection = psycopg2.connect(\n",
    "        user = \"dap\",\n",
    "        password = \"dap\",\n",
    "        host = \"192.168.56.30\",\n",
    "        port = \"5432\",\n",
    "        database = \"coviddata1\")\n",
    "    #set isolation level to 0 / autocommit\n",
    "    dbConnection.set_isolation_level(0)\n",
    "    #establish cursor connection\n",
    "    dbCursor = dbConnection.cursor()\n",
    "    #insert values into the table\n",
    "    insertString = \"INSERT INTO covidtablenew VALUES ({},\"+\"'{}',\"+\"{},\"+\"{},\"+\"{},\"+\"{},\"+\"{},\"+\"'{}',\"+\"{})\"\n",
    "    #use csv reader to read the read the values\n",
    "    with open('CleanedCovidData.csv', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        # skip the header\n",
    "        next(reader)\n",
    "        #loop DB cursor to insert all values\n",
    "        for row in reader:\n",
    "            dbCursor.execute(insertString.format(*row)) \n",
    "    #close the cursor\n",
    "    dbCursor.close()\n",
    "#throw except if connection is not established\n",
    "except (Exception , psycopg2.Error) as dbError :\n",
    "    print (\"Error:\", dbError)\n",
    "#close the connection\n",
    "finally:\n",
    "    if(dbConnection): dbConnection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import pandas.io.sql as sqlio\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "#query to display the values grouped by Month\n",
    "sql=\"\"\"SELECT SUM(PopulationCensus16) as Population, SUM(confirmedcovidcases) as Confirmed_Cases,SUM(populationproportioncovidcases) as Population_Proportion_Covid_Cases,Month\n",
    "       FROM covidtablenew\n",
    "       GROUP BY Month;\"\"\"\n",
    "try:\n",
    "    #establish connecting by giving user, password, host IP, and DB name\n",
    "    dbConnection = psycopg2.connect(\n",
    "        user = \"dap\",\n",
    "        password = \"dap\",\n",
    "        host = \"192.168.56.30\",\n",
    "        port = \"5432\",\n",
    "        database = \"coviddata1\")\n",
    "    \n",
    "    #read and execute SQL query\n",
    "    cov_df_dataframe1 = sqlio.read_sql_query(sql, dbConnection)  \n",
    "\n",
    "\n",
    "#throw except if connection is not established\n",
    "except (Exception , psycopg2.Error) as dbError :\n",
    "    print (\"Error\", dbError)\n",
    "\n",
    "#close the connection\n",
    "finally:\n",
    "    if(dbConnection): dbConnection.close()\n",
    "cov_df_dataframe1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import pandas.io.sql as sqlio\n",
    "import psycopg2\n",
    "\n",
    "#query to display the values grouped by CountyName\n",
    "sql=\"\"\"SELECT SUM(PopulationCensus16) as Population, SUM(confirmedcovidcases) as Confirmed_Cases,SUM(populationproportioncovidcases) as Population_Proportion_Covid_Cases, CountyName\n",
    "       FROM covidtablenew\n",
    "       GROUP BY CountyName;\"\"\"\n",
    "try:\n",
    "    #establish connecting by giving user, password, host IP, and DB name\n",
    "    dbConnection = psycopg2.connect(\n",
    "        user = \"dap\",\n",
    "        password = \"dap\",\n",
    "        host = \"192.168.56.30\",\n",
    "        port = \"5432\",\n",
    "        database = \"coviddata1\")\n",
    "    \n",
    "    #read and execute SQL query\n",
    "    cov_df_dataframe2 = sqlio.read_sql_query(sql, dbConnection)  \n",
    "\n",
    "#throw except if connection is not established\n",
    "except (Exception , psycopg2.Error) as dbError :\n",
    "    print (\"Error\", dbError)\n",
    "\n",
    "#close the connection\n",
    "finally:\n",
    "    if(dbConnection): dbConnection.close()\n",
    "cov_df_dataframe2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations using matplotlib, seaborn and plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing visualization libraries seaborn and matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#set the size of the plot\n",
    "plt.figure(figsize=(26, 6))\n",
    "#bar plot\n",
    "sns.barplot(x = cov_df_dataframe2['countyname'], y = cov_df_dataframe2['confirmed_cases'])\n",
    "#plot title\n",
    "plt.title('Confirmed Covid Cases by County', fontsize=20)\n",
    "#Y-label\n",
    "plt.ylabel('Number of Confirmed Cases ')\n",
    "#X-label\n",
    "plt.xlabel('Months ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization using Plotly\n",
    "\n",
    "#import plotly subplots\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "#assign dataframe values to object\n",
    "covdata = cov_df_dataframe1\n",
    "\n",
    "#define the number and placement of subplots\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Monthly Population\", \"Monthly Confirmed Cases\"))\n",
    "#subplot 1\n",
    "fig.add_trace(go.Scatter(x=covdata.index, y=covdata.population,mode=\"lines\",name=\"population\"),row=1, col=1)\n",
    "#subplot 2\n",
    "fig.add_trace(go.Scatter(x=covdata.index, y=covdata.confirmed_cases,mode=\"lines\",name=\"confirmed_cases\"),row=1, col=2)\n",
    "#show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bar plot shows the confirmed monthly covid cases\n",
    "fig = go.Figure(data=[go.Bar(x=cov_df_dataframe1['month'], y=cov_df_dataframe1['population_proportion_covid_cases'], marker_color=['darkblue','blue','blueviolet','lightblue','indigo','yellowgreen','grey','orange','limegreen','olive','lightyellow'])])\n",
    "#title of plot\n",
    "fig.update_layout(title_text=\"Population Proportion Covid Cases by Month\")\n",
    "#display plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pymongo\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pymongo import MongoClient\n",
    "import pymongo as pym\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import psycopg2\n",
    "import plotly.offline as po\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve as retrieve\n",
    "url_common = 'https://data.smartdublin.ie/dataset/5cb924bb-b0b1-44c3-a51a-7144bebdd52c/resource/'\n",
    "url_month10 = url_common + '43f8bc7a-a717-4df0-9f94-569b55f8d430/download/dcc-summary-tonnages-01.10-to-31.10.20csv.csv'\n",
    "retrieve(url_month10,'Data10.csv')\n",
    "\n",
    "url_month9 = url_common + 'e437075a-fb91-476d-aef9-d923ada5d550/download/dcc-summary-tonnages-01.09-to-30.09.20csv.csv'\n",
    "retrieve(url_month9,'Data9.csv')\n",
    "\n",
    "url_month8 = url_common + '8acec3b0-2fb5-450c-b177-6cc1bb235090/download/dcc-summary-tonnages-01.08-to-31.08.20csv.csv'\n",
    "retrieve(url_month8,'Data8.csv')\n",
    "\n",
    "url_month7 = url_common + 'ccb3d773-3876-4eac-8fe9-c32fb1e31e0d/download/dcc-summary-tonnages-01.07-to-31.07.20csv.csv'\n",
    "retrieve(url_month7,'Data7.csv')\n",
    "\n",
    "url_month6 = url_common + '71b11b9d-a8e7-4058-a0e7-70408e30a05b/download/dcc-summary-tonnages-01.06-to-30.06.20csv.csv'\n",
    "retrieve(url_month6,'Data6.csv')\n",
    "\n",
    "url_month5 = url_common + '9fe542ff-8d4f-4fb8-9609-da4cfa776876/download/dcc-summary-tonnages-01.05-to-31.05.20csv.csv'\n",
    "retrieve(url_month5,'Data5.csv')\n",
    "\n",
    "url_month4 = url_common + '18db26b7-76da-4a6f-95ee-1e2c4d9eb987/download/dcc-summary-tonnages-01.04-to-30.04.20csv.csv'\n",
    "retrieve(url_month4,'Data4.csv')\n",
    "\n",
    "url_month3 = url_common + '24459cec-4e03-4051-96ae-462365829edb/download/dcc-summary-tonnages-01.03-to-31.03.20csv.csv'\n",
    "retrieve(url_month3,'Data3.csv')\n",
    "\n",
    "url_month2 = url_common + 'afc75f33-d826-4101-a997-442f10a19932/download/dcc-summary-tonnages-01.02-to-29.02.20csv.csv'\n",
    "retrieve(url_month2,'Data2.csv')\n",
    "\n",
    "url_month1 = url_common + '69c3a108-73cb-4707-bd7a-1625c4e7c259/download/dcc-summary-tonnages-01.01-to-31.01.20csv.csv'\n",
    "retrieve(url_month1,'Data1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.getcwd()\n",
    "files = glob.glob(os.path.join(path, \"Data*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_file = (pd.read_csv(i, sep=',') for i in files)\n",
    "merged_df = pd.concat(each_file, ignore_index=True)\n",
    "merged_df.to_csv(\"Finalfile.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()\n",
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Finalfile.csv\",\"r\") as f:\n",
    "    reader= csv.reader(f)\n",
    "    next(reader)\n",
    "    final_data=[]\n",
    "    for row in reader:\n",
    "        final_data.append({\"Ticket No\":row[0],\"Address Name\":row[1],\"Date\":row[2],\"Product\":row[3],\"Qty\":row[4],\"Tare Weight\":row[5],\"Gross Weight\":row[6],\"Net Weight\":row[7]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Finalfile.json\",\"w\") as f:\n",
    "    json.dump(final_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('192.168.56.30', 27017)\n",
    "db = client['WastDAP']\n",
    "waste_collection = db['Waste_DB']\n",
    "db.Waste_DB.delete_many({ });\n",
    "result = waste_collection.insert_many(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_cursor = waste_collection.find()\n",
    "waste_datalist = list(waste_cursor)\n",
    "waste_df = pd.DataFrame(waste_datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_df.isnull().sum()# to check is there any null value present value in our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_df.isnull().any()#To check weather is there any empty values or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 2learn how to remove na values\n",
    "waste_df = waste_df.dropna(how = \"all\")#it will drop row if all the row have NA values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_df = waste_df.fillna(method = 'bfill')#this will copy below coloumn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_df.drop('_id',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_df['Date'] = pd.to_datetime(waste_df['Date'])\n",
    "waste_df['OnlyDate'] = waste_df['Date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_df.drop(['Date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_df.rename(columns = {'OnlyDate': 'Date'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "waste_df['Date'] = pd.to_datetime(waste_df['Date'])\n",
    "waste_df['Month'] = waste_df['Date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_df.to_csv('CleanedGarbageData.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dbConnection = psycopg2.connect(\n",
    "    user = \"dap\",\n",
    "    password = \"dap\",\n",
    "    host = \"192.168.56.30\",\n",
    "    port = \"5432\",\n",
    "    database = \"postgres\")\n",
    "    dbConnection.set_isolation_level(0) # AUTOCOMMIT\n",
    "    dbCursor = dbConnection.cursor()\n",
    "    dbCursor.execute(\"DROP DATABASE IF EXISTS Garbage2;\")\n",
    "    dbCursor.execute('CREATE DATABASE Garbage2;')\n",
    "    dbCursor.close()\n",
    "except (Exception , psycopg2.Error) as dbError :\n",
    "    print (\"Error while connecting to PostgreSQL\", dbError)\n",
    "finally:\n",
    "    if(dbConnection): dbConnection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createString = \"\"\"\n",
    "CREATE TABLE GarbageData(\n",
    "Ticket_No integer PRIMARY KEY,\n",
    "Address varchar(255),\n",
    "Product varchar(255),\n",
    "Qty numeric(10,1),\n",
    "Tare_Weight numeric(10,1),\n",
    "Gross_Weight numeric(10,1),\n",
    "Net_Weight numeric(10,1),\n",
    "Date timestamp,\n",
    "Month numeric(10,1)\n",
    ");\n",
    "\"\"\"\n",
    "try:\n",
    "    dbConnection = psycopg2.connect(\n",
    "    user = \"dap\",\n",
    "    password = \"dap\",\n",
    "    host = \"192.168.56.30\",\n",
    "    port = \"5432\",\n",
    "    database = \"garbage2\")\n",
    "    dbConnection.set_isolation_level(0) # AUTOCOMMIT\n",
    "    dbCursor = dbConnection.cursor()\n",
    "    dbCursor.execute(\"DROP TABLE IF EXISTS GarbageData\")\n",
    "    dbCursor.execute(createString)\n",
    "    dbCursor.close()\n",
    "except (Exception , psycopg2.Error) as dbError :\n",
    "    print (\"Error while connecting to PostgreSQL\", dbError)\n",
    "finally:\n",
    "    if(dbConnection): dbConnection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.getcwd()\n",
    "try:\n",
    "    dbConnection = psycopg2.connect(\n",
    "        user = \"dap\",\n",
    "        password = \"dap\",\n",
    "        host = \"192.168.56.30\",\n",
    "        port = \"5432\",\n",
    "        database = \"garbage2\")\n",
    "    dbConnection.set_isolation_level(0) # AUTOCOMMIT\n",
    "    dbCursor = dbConnection.cursor()\n",
    "    insertString = \"INSERT INTO GarbageData VALUES ({},\"+\"'{}',\"+\"'{}',\"+\"{},\"+\"{},\"+\"{},\"+\"{},\"+\"'{}',\"+\"{})\"\n",
    "    with open(path + '\\CleanedGarbageData.csv','r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader) # skip the header\n",
    "        for row in reader:\n",
    "            dbCursor.execute(insertString.format(*row))\n",
    "    dbConnection.commit()\n",
    "    dbCursor.close()\n",
    "except (Exception , psycopg2.Error) as dbError :\n",
    "    print (\"Error:\", dbError)\n",
    "finally:\n",
    "    if(dbConnection): dbConnection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas.io.sql as sqlio\n",
    "query = \"\"\" SELECT *  FROM GarbageData where month =10;\"\"\"\n",
    "try:\n",
    "    dbConnection = psycopg2.connect(\n",
    "        user = \"dap\",\n",
    "        password = \"dap\",\n",
    "        host = \"192.168.56.30\",\n",
    "        port = \"5432\",\n",
    "        database = \"garbage2\")\n",
    "    dataframe2 = sqlio.read_sql_query(query, dbConnection)\n",
    "except (Exception , psycopg2.Error) as dbError :\n",
    "    print (\"Error:\", dbError)\n",
    "finally:\n",
    "    if(dbConnection): dbConnection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "sns.barplot(dataframe2['net_weight'],dataframe2['address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dataframe2['gross_weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(dataframe2['gross_weight'],dataframe2['qty'],kind=\"hex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(dataframe2['date'],dataframe2['net_weight'],kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dataframe2[['gross_weight','net_weight','tare_weight']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='gross_weight',y='qty', data= dataframe2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs #WebScaraping\n",
    "import requests\n",
    "from urllib.request import urlretrieve as retrieve\n",
    "url='https://cli.fusio.net/cli/climate_data/webdata/dly3723.csv'\n",
    "retrieve (url, 'weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.read_csv(\"weather.csv\", skiprows=24)\n",
    "weather_df.to_csv(\"weather1.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"weather1.csv\",\"r\") as f:\n",
    "    reader= csv.reader(f)\n",
    "    next(reader)\n",
    "    data=[]\n",
    "    for row in reader:\n",
    "        data.append({\"Date\":row[0],\"ind\":row[1],\"maxtp\":row[2],\"ind\":row[3],\"mintp\":row[4],\"igmin\":row[5],\"gmin\":row[6],\"ind\":row[7],\"rain\":row[8],\"cbl\":row[9],\"wdsp\":row[10],\"ind\":row[11],\"hm\":row[12],\"ind\":row[13],\"ddhm\":row[14],\"ind\":row[15],\"hg\":row[16],\"sun\":row[17],\"dos\":row[18]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"rain.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pymongo as pm\n",
    "\n",
    "client = MongoClient('192.168.56.30', 27017)\n",
    "db = client['DAP']\n",
    "ac_col = db['Weatherforecast']\n",
    "db.Weatherforecast.delete_many({ });\n",
    "result = ac_col.insert_many(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_data = ac_col.find()\n",
    "ac_datalist = list(ac_data)\n",
    "ac_df = pd.DataFrame(ac_datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_df=ac_df.iloc[20454:20788]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_df = cd_df[['maxtp','mintp','rain','wdsp','Date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_df.to_csv(\"CleanedWeatherData.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrated Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data for Joining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepCovidData():\n",
    "    \"\"\" \n",
    "    This function will prepare COVID-19 data for joining.\n",
    "    Input: Input is Cleaned Covid Data CSV file.\n",
    "    Output: Output will be cleaned dataframe ready for joining.\n",
    "    \"\"\"\n",
    "    # Importing Necessary Libraries\n",
    "    import pandas as pd\n",
    "    import plotly.express as px\n",
    "    \n",
    "    # Reading Cleaned CSV data in dataframe\n",
    "    Covid_df = pd.read_csv(\"CleanedCovidData.csv\")\n",
    "    \n",
    "    # Calculating per day cases by taking difference of consecutive days\n",
    "    Covid_df['PerDayCases'] = Covid_df.groupby(['CountyName'])['ConfirmedCovidCases'].diff(1).fillna(0).clip(lower=0)\n",
    "    # Dropping redundant columns \n",
    "    Covid_df = Covid_df.drop(['ConfirmedCovidCases','PopulationProportionCovidCases'], axis=1)\n",
    "\n",
    "    ## Data Visualization 0\n",
    "    # Grouping COVID data and taking sum of confirmed cases\n",
    "    Covid_df1 = Covid_df.groupby(['CountyName','Lat','Long'])[['PerDayCases']].sum().reset_index()\n",
    "    # Creating map using plotly\n",
    "    fig0 = px.scatter_mapbox(Covid_df1, lat=\"Lat\", lon=\"Long\", color=\"CountyName\", size=\"PerDayCases\",\n",
    "                      color_continuous_scale=px.colors.cyclical.IceFire, size_max=25, zoom=6,\n",
    "                      mapbox_style=\"carto-positron\")\n",
    "    # Adding title to the figure\n",
    "    fig0.update_layout(title_text=\"Plot 0: Total COVID-19 cases as per the County in Ireland\")\n",
    "    # To display the figure\n",
    "    fig0.show()\n",
    "    \n",
    "    # Grouping COVID-19 data by Date and taking sum of COVID-19 cases.\n",
    "    Covid_df = Covid_df.groupby('Date')[['PerDayCases']].sum().reset_index()\n",
    "    # Replacing / in the Date column with - in order to match Date format in all datasets.\n",
    "    Covid_df['Date'] = Covid_df['Date'].replace({'/': '-'}, regex=True)\n",
    "    Covid_df.to_csv(\"CleanedCovidNew.csv\", index=None)\n",
    "    # Returning cleaned Covid dataframe\n",
    "    return Covid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepCovidData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepJoin(Covid_df):\n",
    "    \"\"\" \n",
    "    This function will prepare remaining 3 datasets for joining.\n",
    "    Input: Input is all cleaned CSV files and cleaned COVID dataframe.\n",
    "    Output: Output will be cleaned and merged dataframe.\n",
    "    \"\"\"\n",
    "    # Importing Necessary Libraries\n",
    "    import pandas as pd\n",
    "    import datetime\n",
    "    \n",
    "    # Reading all cleaned CSV files in dataframe\n",
    "    Garbage_df = pd.read_csv(\"CleanedGarbageData.csv\")\n",
    "    Weather_df = pd.read_csv(\"CleanedWeatherData.csv\")\n",
    "    Hosp_df = pd.read_csv(\"CleanedHospitalData.csv\")\n",
    "\n",
    "    # Grouping Garbage data by Date and taking mean of Quantity and Net Weight.\n",
    "    Garbage_df = Garbage_df.groupby('Date')[['Qty','Net Weight']].mean().reset_index()\n",
    "    # Renaming Columns of Garbage dataset.\n",
    "    Garbage_df.rename(columns = {'Net Weight': 'NetWgt'}, inplace=True)\n",
    "    Garbage_df['Date'] = pd.to_datetime(Garbage_df['Date'])\n",
    "\n",
    "    # Renaming Columns of Weather dataset.\n",
    "    Weather_df.rename(columns = {'maxtp': 'MaxTmp','mintp':'MinTmp','rain':'Rain','wdsp':'WindSpd'}, inplace=True)\n",
    "    Weather_df['Date'] = pd.to_datetime(Weather_df['Date'])\n",
    "    \n",
    "    Covid_df['Date'] = pd.to_datetime(Covid_df['Date'])\n",
    "    \n",
    "    # Merging COVID-19, Garbage and Weather datasets.\n",
    "    merge1 = Covid_df.merge(Garbage_df[['Qty','NetWgt','Date']], on = 'Date').merge(Weather_df, on = 'Date')\n",
    "    # Converting Date column as datatime object.\n",
    "    merge1['Date'] = pd.to_datetime(merge1['Date'])\n",
    "    # Creating a month column from Date column which is a datetime object.\n",
    "    merge1['Month'] = merge1['Date'].dt.month\n",
    "    # Finally merging Hospital referral dataset with above merged dataframe\n",
    "    final_df = merge1.merge(Hosp_df, on = 'Month')\n",
    "    # Converting Date column as datatime object.\n",
    "    final_df['Date'] = pd.to_datetime(final_df['Date'])\n",
    "    final_df['Date'] = final_df[\"Date\"].dt.strftime(\"%d-%m-%Y\")\n",
    "    # Saving the final merged data as CSV file\n",
    "    final_df.to_csv(\"Final_merged.csv\", index=None)\n",
    "    # Returning final merged dataframe.\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataVisual(final_df):\n",
    "    \"\"\" \n",
    "    This function will visualize data from final merged dataframe.\n",
    "    Input: Input is final merged dataframe. \n",
    "    Output: Output will be visualizations from merged dataframe.\n",
    "    \"\"\"\n",
    "    # Importing Necessary Libraries\n",
    "    import pandas as pd\n",
    "    import plotly.express as px\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    ## Data Visualization 1\n",
    "    # Grouping by Month and taking sum of COVID-19 cases\n",
    "    final_df1 = final_df.groupby(['Month'])['PerDayCases'].sum().reset_index()\n",
    "#     final_df1.head()\n",
    "    # Creating a barplot of COVID-19 cases and Month\n",
    "    sns.barplot(final_df1['Month'],final_df1['PerDayCases'])\n",
    "    # Giving title to the figure\n",
    "    plt.title(\"Plot 1: Bar plot of COVID-19 cases as per the Month\")\n",
    "    # To display the figure\n",
    "    plt.show()\n",
    "\n",
    "    ## Data Visualization 2\n",
    "    # Grouping data by hospital department and Windspeed and taking sum of COVID-19 cases.\n",
    "    final_df2 = final_df.groupby(['Hosp_Dept','WindSpd'])['PerDayCases'].sum().reset_index()\n",
    "    # Creating a Scatter plot\n",
    "    fig2 = px.scatter(final_df2, x=\"PerDayCases\", y=\"WindSpd\", color=\"Hosp_Dept\", hover_data = final_df2.columns)\n",
    "    # Adding title to the figure and labelling x and y axis\n",
    "    fig2.update_layout(title = 'Plot 2: Scatterplot of COVID-19 cases with Wind Speed',\n",
    "                      xaxis_title = 'Confirmed Cases', yaxis_title = 'Wind Speed')\n",
    "    # To display the figure\n",
    "    fig2.show()\n",
    "\n",
    "    ## Data Visualization 3\n",
    "    # Grouping by Month, Net Weight & Total Referrals and taking sum of Covid cases\n",
    "    final_df3 = final_df.groupby(['NetWgt','TotalRef','Month'])[['PerDayCases']].sum().reset_index()\n",
    "    # Creating a scatter plot between Covid cases and total referrals\n",
    "    fig3 = px.scatter(final_df3, x=\"PerDayCases\", y=\"TotalRef\", color='NetWgt', marginal_y=\"violin\",\n",
    "               marginal_x=\"box\", trendline=\"ols\", template=\"simple_white\")\n",
    "    # Adding title to the figure and labelling x and y axis\n",
    "    fig3.update_layout(title = 'Plot 3: Scatterplot of COVID-19 cases and Total Referrals',\n",
    "                      xaxis_title = 'Confirmed COVID-19 Cases', yaxis_title = 'Total Referrals')\n",
    "    # To display the plot\n",
    "    fig3.show()\n",
    "\n",
    "    ## Data Visualization 4\n",
    "    # Grouping by Hospital Department, Referral Type & Month and taking sum of Covid cases\n",
    "    final_df4 = final_df.groupby(['Hosp_Dept','RefType','Month'])['PerDayCases'].sum().reset_index()\n",
    "    # Creating a bar plot between Covid cases and Hospital department\n",
    "    fig4 = px.bar(final_df4, x=\"Hosp_Dept\", y=\"PerDayCases\", color=\"RefType\", barmode=\"group\", hover_data=final_df4.columns)\n",
    "    # Adding title to the figure and labelling x and y axis\n",
    "    fig4.update_layout(title = 'Plot 4: Barplot of Hospital department and COVID-19 cases as per the Month and Referral Type',\n",
    "                      xaxis_title = 'Hospital department', yaxis_title = 'COVID-19 cases')\n",
    "    # To display the plot\n",
    "    fig4.show()\n",
    "\n",
    "    ## Data Visualization 5\n",
    "    # Grouping by Hospital Department, Referral Type & Month and taking sum of Covid cases\n",
    "    final_df5 = final_df.groupby(['Hosp_Dept','RefType','Month'])['PerDayCases'].sum().reset_index()\n",
    "    # Creating a bar plot between Covid cases and Hospital department\n",
    "    fig5 = px.bar(final_df5, x=\"Hosp_Dept\", y=\"PerDayCases\", color=\"RefType\", barmode=\"group\", facet_col=\"Month\")\n",
    "    # Adding title to the figure and labelling x and y axis\n",
    "    fig5.update_layout(title = 'Plot 5: Barplot of Hospital department and COVID-19 cases as per Month', yaxis_title = 'COVID-19 cases')\n",
    "    # To display the plot\n",
    "    fig5.show()\n",
    "\n",
    "    ## Data Visualization 6\n",
    "    # Creating a scatter matrix with respect to Referral Type\n",
    "    fig6 = px.scatter_matrix(final_df, dimensions=[\"PerDayCases\", \"NetWgt\", \"Rain\", \"TotalRef\"], color=\"RefType\")\n",
    "    # Adding title to the figure\n",
    "    fig6.update_layout(title = 'Plot 6: Scatter matrix of different features with respect to Referral Type')\n",
    "    # To display the plot\n",
    "    fig6.show()\n",
    "\n",
    "    ## Data Visualization 7\n",
    "    # Grouping Hospital Department, Referral Type & Net Weight and taking sum of Covid Cases\n",
    "    final_df7 = final_df.groupby(['Hosp_Dept','RefType','Qty'])['PerDayCases'].sum().reset_index()\n",
    "    # Creating a sunburst of Referral Type and Hospital Department with Covid cases\n",
    "    fig7 = px.sunburst(final_df7, path=['RefType', 'Hosp_Dept'], values='PerDayCases', color='Qty')\n",
    "    # Adding title to the figure\n",
    "    fig7.update_layout(title = 'Plot 7: Sunburst of Referral Type and Hospital Department with Covid cases')\n",
    "    # To display the plot\n",
    "    fig7.show()\n",
    "\n",
    "    ## Data Visualization 8\n",
    "    # Creating a tree map of Referral Type and Hospital Department with Covid cases\n",
    "    fig8 = px.treemap(final_df, path=['RefType', 'Hosp_Dept'], values='PerDayCases', color='MaxTmp')\n",
    "    # Adding title to the figure\n",
    "    fig8.update_layout(title = 'Plot 8: Treemap of Referral Type and Hospital Department with Covid cases and Maximum Temperature')\n",
    "    # To display the plot\n",
    "    fig8.show()\n",
    "\n",
    "    ## Data Visualization 9\n",
    "    # Creating a boxplot of COVID cases with month and Referral Type\n",
    "    fig9 = px.box(final_df, x=\"Month\", y=\"PerDayCases\", color=\"RefType\", notched=True)\n",
    "    # Adding title to the figure and labelling x and y axis\n",
    "    fig9.update_layout(title = 'Plot 9: Boxplot of COVID cases with month and Referral Type', \n",
    "                       xaxis_title = 'Month', yaxis_title = 'COVID-19 cases')\n",
    "    # To display the plot\n",
    "    fig9.show()\n",
    "\n",
    "    ## Data Visualization 10\n",
    "    # Creating a Density Contour of COVID cases with Rain and Referral Type\n",
    "    fig10 = px.density_contour(final_df, x=\"PerDayCases\", y=\"Rain\", color=\"RefType\", marginal_x=\"rug\", marginal_y=\"histogram\")\n",
    "    # Adding title to the figure and labelling x and y axis\n",
    "    fig10.update_layout(title = 'Plot 10: Density Contour plot of COVID cases with Rain and Referral Type', \n",
    "                       xaxis_title = 'COVID-19 cases', yaxis_title = 'Rain')\n",
    "    # To display the plot\n",
    "    fig10.show()\n",
    "\n",
    "    ## Data Visualization 11\n",
    "    # Creating a scatter plot of Covid cases with quantity, Month and Hospital Department\n",
    "    fig11 = px.scatter(final_df, x = 'PerDayCases', y = 'Qty', animation_frame='Month', color='Hosp_Dept')\n",
    "    # Adding title to the figure and labelling x and y axis\n",
    "    fig11.update_layout(title = 'Plot 11: Scatterplot of COVID-19 cases with Waste quantity', xaxis_title = 'Confirmed Cases', \n",
    "                        yaxis_title = 'Quantity')\n",
    "    # To display the plot\n",
    "    fig11.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataStats(final_df):\n",
    "    \"\"\" \n",
    "    This function is to check the statistical details like mean, quartile, etc.\n",
    "    Input: Input is final merged dataframe. \n",
    "    Output: Output will be different statistical measures and Pearson's Correlation Coefficient between all numerical features.\n",
    "    \"\"\"\n",
    "    # To check some statistical details such as mean, percentile, etc.\n",
    "    print(\"Statistical details of Merged dataset\")\n",
    "    print(\"------------------------------------------\")\n",
    "    print(final_df.describe())\n",
    "    # To check the Pearson's Correlation Coefficient.\n",
    "    print(\"\\n Pearson's Correlation Coefficient for all numerical features present in Merged dataset\")\n",
    "    print(\"-----------------------------------------------------------------------------------------\")\n",
    "    print(final_df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainExecution():\n",
    "    \"\"\" \n",
    "    This is the main function from where the execution of program starts.\n",
    "    Input: Input is cleaned CSV files of all the datasets.\n",
    "    Output: Output will be analysis and visulizations performed on joined dataframe.\n",
    "    \"\"\"\n",
    "    cv_df = prepCovidData()\n",
    "    fnl_df = prepJoin(cv_df)\n",
    "    dataVisual(fnl_df)\n",
    "    dataStats(fnl_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the main function. This is point from where the execution of program starts.\n",
    "mainExecution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ Insights: \n",
    "- Plot 0: This plot shows the number of confirm COVID-19 cases as per the County in Ireland. The size of marker indicates the number of confirm cases. As the number of such instance increases the size of marker also increases. The biggest marker is on Dublin which infers Dublin has the maximum number of COVID-19 cases in Ireland.\n",
    "- Plot 1: This plot shows the number of confirm COVID-19 cases per month in Ireland. We can see maximum number of cases was in the month of October followed by the month of April. Also, the month of July had least number of COVID-19 cases.\n",
    "- Plot 2: This plot shows the scatter plot between Wind Speed and COVID-19 cases by different hospital department. This plot shows that there is not much relationship between Wind Speed and COVID-19 cases. \n",
    "- Plot 3: This plot also shows the scatter plot between COVID-19 cases and Total referrals done in Hospital. We can infer that there is some positive association between these two variables. We can see Ordinary Least Squares (OLS) regression trendline which shows the line equation and also R-squared value while hovering.\n",
    "- Plot 4: This plot shows bar plot of the different departments of COVID-19 testing hospital with number of COVID-19 cases. We can deduce that maximum number of cases was from Others department which includes Pregnant Woman, Residential Setting and Prison Inmate. This is followed by General Covid Test department which has two types of Referrals i.e., General Referrals and Covid Testing Referrals. The lowest cases was from individuals who was unable to travel.\n",
    "- Plot 5: This plot shows bar plot of the different departments of COVID-19 testing hospital with number of COVID-19 cases according to Month. We can say that the maximum number of cases was in the month of October which was of General Covid test and Others hospital department. \n",
    "- Plot 6: This plot is the scatter matrix which shows the relationship between different parameters such as Total Referrals, Rain, Net Weight of Garbage collection and sum of COVID-19 cases differentiated by Referral types. We can infer that there is some considerable association between COVID-19 cases and Net Weight of Waste collection and also between COVID-19 cases and Total Referrals. Also, there is not much relationship between COVID-19 cases and Rain.\n",
    "- Plot 7: From this sunburst plot we can conclude that maximum number of COVID-19 cases was from Others hospital department which includes Pregnant Woman, Residential Setting and Prison Inmate and is part of General Referral. This is followed by General Covid test which comes under Covid Testing Referrals. Moreover, we can also infer that maximum Quantity of waste collection is from Unable to travel Category though minimum number of COVID-19 cases is from this department.\n",
    "- Plot 8: This tree map gives the overall idea of Hospital departments having maximum number of COVID-19 cases. The different colours show how number of cases varies with respect to Maximum Temperature. We can see that maximum temperature was in the case of General Covid test which is the part of Covid testing referrals having second highest number of Covid cases.\n",
    "- Plot 9: This box plot shows that the maximum number of COVID-19 cases was in the month of October. Also, the Covid testing referrals was started from the month of May. The least number of cases was in the month of June.\n",
    "- Plot 10: This density contour plot shows the range of both the referrals types in the first part. The second part also shows the variability of COVID-19 cases with respect to Rain and the third part shows the histogram for the same. We can say that General Referral type has higher variability as compared to Covid testing Referrals whereas Rain does not have much relation with COVID-19 cases.\n",
    "- Plot 11: This scatter plot shows association between COVID-19 cases and Quantity of Waste Collection with Month. We can infer that maximum number of instances was from Others department and is in the month of October. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
